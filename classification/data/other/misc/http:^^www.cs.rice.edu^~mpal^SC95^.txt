date tue jan gmt server ncsa content type text html last modified mon dec gmt content length fortran d compiler overview fortran d compiler overview project leaders john mellor crummey vikram adve participants zoran budimlic alan carle kevin cureton gil hansen ken kennedy charles koelbel bo lu collin mccurdy nat mcintosh dejan mircevski nenad nedeljkovic mike paleczny ajay sethi yoshiki seo lisa thomas lei zhou parallel compiler tools group center research parallel computation rice university group mission develop integrated compiler tools support effective machine independent parallel programming contents compiler goals fortran d language fortran d compiler organization compiled examples compiler goals serve extensible platform experimental research compilation techniques programming tools full applications including unified treatment regular irregular problems global strategies computation partitioning parallelism enhancing latency hiding transformations whole program compilation interprocedural transformations code generation strategies approach hand tuned performance architecture independent compilation based machine models message passing shared memory hybrid communication models optimization presence resource constraints programming tools fully support abstract high level programming models fortran d language fortran d designed support research data parallel programming high performance fortran hpf explore extensions would broaden hpf applicability enhance performance features fortran fortran array syntax forall allocate high performance fortran hpf data mapping directives regular problems align distribute realign redistribute template processors independent home value based data mapping directives support irregular problems experimental support development parallel input output including core array management complex data structures structured use task parallelism fortran d compiler organization front end parallelism preliminary communication placement computation partitioning communication placement communication refinement code generation front end purpose interpret hpf directives compute directives affecting statement reference directive processing semantic analysis directives program infer canonical synthetic layout directives program variables unmentioned program directives intraprocedural flow sensitive propagation re align re distribute statements array references limitations november interprocedural propagation layout information preliminary communication placement purpose provide feedback computation partitioner conservatively communication might needed strategy conservatively assume references non replicated variables may need communication hoist communication reference outermost loop level possible respecting data dependences reference subscripts conservatively prevent communication hoisted non loop iterative constructs limitations november placement independent resource constraints support pipelining communication achieve partial parallelism lacks dataflow placement optimization eliminate partial redundancies hide communication latency inspector placement irregular data accesses computation partitioning selection purpose framework evaluate select several computation partitioning alternatives restricted owner computes rule approach explicitly enumerate candidate partitioning choices use explicit cost estimation select best partitioning enumerate candidate cp choices loop nest set loop nests example refine communication information candidate cp estimate performance candidate cp load balance unimplemented communication overhead propagate computation partitionings statements computations involving privatizable variables limitations november load balance considered ignores message coalescing across loop nests communication cost estimates simplistic requires constant loop bounds simplistic handling symbolics straightforward communication refinement purpose given computation partition choice cp compute projection conservatively placed communication w r cp example eliminate communication references local cp eliminate redundant communication coalescing determine communication pattern perform message coalescing optimization limitations november assume one single reaching layout per reference conservative unless single processors statement perfect alignment number distributed dimensions communication pattern recognition somewhat limited dataflow analysis eliminating partial redundancies latency hiding yet fully place code generation principal functionality source running example computation partitioning transformations reduce loop bounds insert guards necessary example separate loop iterations might access non local values minimize overhead runtime locality checks example communication generation storage management compute data sets send recv processor pairs generate code pack unpack buffers send recv data example generate run time dynamic storage management cope dynamic layouts example localize linearize subscripts example current strategy except storage management code generation tasks require heavy manipulation integer sets especially compiling regular applications distributed memory machines examples data send recieve particular reference given computation partition processor loop iterations access local non local data current implementation uses omega library university maryland arbitrary integer sets rich language mappings sets almost complete set operations sets mappings union intersection difference inverse composition good code generation optimization code generation slow limited support symbolics limitations november run time resolution guards currently handle one processors per dynamic statement instance lack library support dynamic remapping current localization linearization strategy produces general slow code compiled examples simple d shift kernel hpf f mpi jacobi iteration hpf f mpi livermore explicit hydrodynamics kernel hpf f mpi non owner computes partitioning fragment http www cs rice edu mpal sc index html